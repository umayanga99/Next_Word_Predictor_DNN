{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1JEWqk8iFdeU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#path for the file\n",
        "file_path=\"pizza.txt\"\n",
        "\n",
        "#reading the file\n",
        "with open (file_path,\"r\") as file:\n",
        "    text=file.read()\n",
        "\n",
        "    #spliting the data input according to the regex provided\n",
        "    data=[sentence.strip() for sentence in re.split(r'(?<=[.!?]\\s+)',text) if sentence.strip()]\n",
        "# print(data)\n"
      ],
      "metadata": {
        "id": "bYVz0ExcF2b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making an instance of the tokenizer\n",
        "# tokenizer is used to map each unique word into uniqye integer\n",
        "tokenizer=Tokenizer()\n",
        "\n",
        "#scans through all the texts and keeps track of the frequency of each word in the texts\n",
        "tokenizer.fit_on_texts(data)\n",
        "# +1 is to act as a place holder for out-of-vocabulary\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "# print(total_words)"
      ],
      "metadata": {
        "id": "JOxK05twLtrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "#making n-gram list for input text\n",
        "for line in data:\n",
        "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1,len(token_list)):\n",
        "        n_gram_sequence=token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "# print(input_sequences)"
      ],
      "metadata": {
        "id": "V712_kaBQTBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len=max([len(seq) for seq in input_sequences],default=0)\n",
        "# padding the sequence to take the every input to the same length\n",
        "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding=\"pre\"))\n",
        "\n",
        "# choosing X,y\n",
        "# X = choosing all the list inside the list except the last item of each list\n",
        "# y = choosing all the last item of the each list\n",
        "# by this we can predict the next word(Integer) using the sequence of words (Integers)\n",
        "X,y=input_sequences[:,:-1],input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "yx7Co5dJnS9l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "EAoKXC02-uYY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "#adding a embedding layer to the sequendial model\n",
        "#translates high-dimensional data (like one-hot encoded vectors) into a lower-dimensional space.\n",
        "model.add(Embedding(total_words,10,input_length=max_sequence_len-1))\n",
        "#adding Long short term memory(RNN) to the model with 128 neurons\n",
        "model.add(LSTM(128))\n",
        "#adding dense(Fully connected layer to the model)\n",
        "model.add(Dense(total_words,activation=\"softmax\"))\n",
        "#\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "PiAgBI5hAg90"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}